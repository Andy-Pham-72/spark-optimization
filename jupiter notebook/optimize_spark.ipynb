{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Query Plan Optimization\n",
    "\n",
    "Suppose we want to compose query in which we get for each question also the number of answers to this question for each month. See the query below which does that in a suboptimal way and try to rewrite it to achieve a more optimal plan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, month, broadcast\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('Optimize I').getOrCreate()\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "answers_input_path = os.path.join(project_path, '/Volumes/Moon/SpringBoard/spark-optimization/data/answers')\n",
    "\n",
    "questions_input_path = os.path.join(project_path, '/Volumes/Moon/SpringBoard/spark-optimization/data/questions')\n",
    "\n",
    "answersDF = spark.read.option('path', answers_input_path).load()\n",
    "\n",
    "questionsDF = spark.read.option('path', questions_input_path).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+--------+-------+-----+\n",
      "|question_id|answer_id|       creation_date|comments|user_id|score|\n",
      "+-----------+---------+--------------------+--------+-------+-----+\n",
      "|     226592|   226595|2015-12-29 18:46:...|       3|  82798|    2|\n",
      "|     388057|   388062|2018-02-22 13:52:...|       8|    520|   21|\n",
      "|     293286|   293305|2016-11-17 16:35:...|       0|  47472|    2|\n",
      "|     442499|   442503|2018-11-22 01:34:...|       0| 137289|    0|\n",
      "|     293009|   293031|2016-11-16 08:36:...|       0|  83721|    0|\n",
      "|     395532|   395537|2018-03-25 01:51:...|       0|   1325|    0|\n",
      "|     329826|   329843|2017-04-29 11:42:...|       4|    520|    1|\n",
      "|     294710|   295061|2016-11-26 20:29:...|       2| 114696|    2|\n",
      "|     291910|   291917|2016-11-10 05:56:...|       0| 114696|    2|\n",
      "|     372382|   372394|2017-12-03 21:17:...|       0| 172328|    0|\n",
      "|     178387|   178394|2015-04-25 13:31:...|       6|  62726|    0|\n",
      "|     393947|   393948|2018-03-17 18:22:...|       0| 165299|    9|\n",
      "|     432001|   432696|2018-10-05 04:47:...|       1| 102218|    0|\n",
      "|     322740|   322746|2017-03-31 14:10:...|       0|    392|    0|\n",
      "|     397003|   397008|2018-04-01 07:31:...|       1| 189394|    6|\n",
      "|     223572|   223628|2015-12-12 00:40:...|       0|  94772|   -1|\n",
      "|     220328|   220331|2015-11-24 10:57:...|       3|  92883|    1|\n",
      "|     176400|   176491|2015-04-16 09:13:...|       0|  40330|    0|\n",
      "|     265167|   265179|2016-06-28 07:58:...|       0|  46790|    0|\n",
      "|     309103|   309105|2017-02-01 12:00:...|       2|  89597|    2|\n",
      "+-----------+---------+--------------------+--------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the answersDF\n",
    "answersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+------------------+--------+-------+-----+\n",
      "|question_id|                tags|       creation_date|               title|accepted_answer_id|comments|user_id|views|\n",
      "+-----------+--------------------+--------------------+--------------------+------------------+--------+-------+-----+\n",
      "|     382738|[optics, waves, f...|2018-01-28 02:22:...|What is the pseud...|            382772|       0|  76347|   32|\n",
      "|     370717|[field-theory, de...|2017-11-25 04:09:...|What is the defin...|              null|       1|  75085|   82|\n",
      "|     339944|[general-relativi...|2017-06-17 16:32:...|Could gravitation...|              null|      13| 116137|  333|\n",
      "|     233852|[homework-and-exe...|2016-02-04 16:19:...|When does travell...|              null|       9|  95831|  185|\n",
      "|     294165|[quantum-mechanic...|2016-11-22 06:39:...|Time-dependent qu...|              null|       1| 118807|   56|\n",
      "|     173819|[homework-and-exe...|2015-04-02 11:56:...|Finding Magnetic ...|              null|       5|  76767| 3709|\n",
      "|     265198|    [thermodynamics]|2016-06-28 10:56:...|Physical meaning ...|              null|       2|  65035| 1211|\n",
      "|     175015|[quantum-mechanic...|2015-04-08 21:24:...|Understanding a m...|              null|       1|  76155|  326|\n",
      "|     413973|[quantum-mechanic...|2018-06-27 09:29:...|Incorporate spino...|              null|       3| 167682|   81|\n",
      "|     303670|[quantum-field-th...|2017-01-08 01:05:...|A Wilson line pro...|              null|       0| 101968|  184|\n",
      "|     317368|[general-relativi...|2017-03-08 14:53:...|Shouldn't Torsion...|              null|       0|  20427|  305|\n",
      "|     369982|[quantum-mechanic...|2017-11-20 22:11:...|Incompressible in...|              null|       4| 124864|   83|\n",
      "|     239745|[quantum-mechanic...|2016-02-25 03:51:...|Is this correct? ...|            239773|       2|  89821|   78|\n",
      "|     412294|[quantum-mechanic...|2018-06-17 20:46:...|Is electron/photo...|              null|       0|    605|   61|\n",
      "|     437521|[thermodynamics, ...|2018-10-29 02:49:...|Distance Dependen...|              null|       2| 211152|   19|\n",
      "|     289701|[quantum-field-th...|2016-10-29 23:56:...|Generalize QFT wi...|              null|       4|  31922|   49|\n",
      "|     239505|[definition, stab...|2016-02-24 05:51:...|conditions for so...|              null|       3| 102021|  121|\n",
      "|     300744|[electromagnetism...|2016-12-24 13:14:...|Maxwell equations...|            300749|       0| 112190|  171|\n",
      "|     217315|[nuclear-physics,...|2015-11-08 04:13:...|Is the direction ...|              null|       1|  60150| 1749|\n",
      "|     334778|[cosmology, cosmo...|2017-05-22 09:58:...|Why are fluctatio...|            334791|       3| 109312|  110|\n",
      "+-----------+--------------------+--------------------+--------------------+------------------+--------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the questionsDF\n",
    "questionsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|question_id|       creation_date|               title|month|cnt|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    1|  1|\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    2|  1|\n",
      "|     155990|2014-12-31 21:51:...|The abstract spac...|    1|  2|\n",
      "|     155992|2014-12-31 22:44:...|centrifugal force...|    1|  1|\n",
      "|     155993|2014-12-31 22:56:...|How can I estimat...|    1|  1|\n",
      "|     155995|2015-01-01 00:16:...|Why should a solu...|    1|  3|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    1|  2|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    2|  1|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|   11|  1|\n",
      "|     155997|2015-01-01 01:26:...|Why do square sha...|    1|  3|\n",
      "|     155999|2015-01-01 02:01:...|Diagonalizability...|    1|  1|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|    1|  2|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|   11|  1|\n",
      "|     156016|2015-01-01 05:31:...|The interference ...|    1|  1|\n",
      "|     156020|2015-01-01 06:19:...|What is going on ...|    1|  1|\n",
      "|     156021|2015-01-01 06:21:...|How to calculate ...|    2|  1|\n",
      "|     156022|2015-01-01 06:55:...|Advice on Major S...|    1|  1|\n",
      "|     156025|2015-01-01 07:32:...|Deriving the Cano...|    1|  1|\n",
      "|     156026|2015-01-01 07:49:...|Does Bell's inequ...|    1|  3|\n",
      "|     156027|2015-01-01 07:49:...|Deriving X atom f...|    1|  1|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Processing time: 1.6626389026641846 seconds\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [question_id#12L, creation_date#14, title#15, month#94, cnt#110L]\n",
      "   +- BroadcastHashJoin [question_id#12L], [question_id#0L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(question_id#12L)\n",
      "      :  +- FileScan parquet [question_id#12L,creation_date#14,title#15] Batched: true, DataFilters: [isnotnull(question_id#12L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [id=#215]\n",
      "         +- HashAggregate(keys=[question_id#0L, month#94], functions=[count(1)])\n",
      "            +- Exchange hashpartitioning(question_id#0L, month#94, 200), ENSURE_REQUIREMENTS, [id=#212]\n",
      "               +- HashAggregate(keys=[question_id#0L, month#94], functions=[partial_count(1)])\n",
      "                  +- Project [question_id#0L, month(cast(creation_date#2 as date)) AS month#94]\n",
      "                     +- Filter isnotnull(question_id#0L)\n",
      "                        +- FileScan parquet [question_id#0L,creation_date#2] Batched: true, DataFilters: [isnotnull(question_id#0L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Answers aggregation\n",
    "\n",
    "Here we : get number of answers per question per month\n",
    "'''\n",
    "# initialize start_time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "answers_month = answersDF.withColumn('month', month('creation_date')).groupBy('question_id', 'month').agg(count('*').alias('cnt'))\n",
    "\n",
    "resultDF = questionsDF.join(answers_month, 'question_id').select('question_id', 'creation_date', 'title', 'month', 'cnt')\n",
    "\n",
    "resultDF.orderBy('question_id', 'month').show()\n",
    "\n",
    "print(\"Processing time: %s seconds\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "resultDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**:\n",
    "\n",
    "See the query plan of the previous result and rewrite the query to optimize it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 1\n",
    "- We will repartition the answersDF dataframe by `creation_date` column as new column named `month`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|question_id|       creation_date|               title|month|cnt|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    1|  1|\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    2|  1|\n",
      "|     155990|2014-12-31 21:51:...|The abstract spac...|    1|  2|\n",
      "|     155992|2014-12-31 22:44:...|centrifugal force...|    1|  1|\n",
      "|     155993|2014-12-31 22:56:...|How can I estimat...|    1|  1|\n",
      "|     155995|2015-01-01 00:16:...|Why should a solu...|    1|  3|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    1|  2|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    2|  1|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|   11|  1|\n",
      "|     155997|2015-01-01 01:26:...|Why do square sha...|    1|  3|\n",
      "|     155999|2015-01-01 02:01:...|Diagonalizability...|    1|  1|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|    1|  2|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|   11|  1|\n",
      "|     156016|2015-01-01 05:31:...|The interference ...|    1|  1|\n",
      "|     156020|2015-01-01 06:19:...|What is going on ...|    1|  1|\n",
      "|     156021|2015-01-01 06:21:...|How to calculate ...|    2|  1|\n",
      "|     156022|2015-01-01 06:55:...|Advice on Major S...|    1|  1|\n",
      "|     156025|2015-01-01 07:32:...|Deriving the Cano...|    1|  1|\n",
      "|     156026|2015-01-01 07:49:...|Does Bell's inequ...|    1|  3|\n",
      "|     156027|2015-01-01 07:49:...|Deriving X atom f...|    1|  1|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Processing time: 0.7143349647521973 seconds\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [question_id#12L, creation_date#14, title#15, month#152, cnt#168L]\n",
      "   +- BroadcastHashJoin [question_id#12L], [question_id#0L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(question_id#12L)\n",
      "      :  +- FileScan parquet [question_id#12L,creation_date#14,title#15] Batched: true, DataFilters: [isnotnull(question_id#12L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [id=#416]\n",
      "         +- HashAggregate(keys=[question_id#0L, month#152], functions=[count(1)])\n",
      "            +- HashAggregate(keys=[question_id#0L, month#152], functions=[partial_count(1)])\n",
      "               +- Exchange hashpartitioning(month#152, 200), REPARTITION_BY_COL, [id=#408]\n",
      "                  +- Project [question_id#0L, month(cast(creation_date#2 as date)) AS month#152]\n",
      "                     +- Filter isnotnull(question_id#0L)\n",
      "                        +- FileScan parquet [question_id#0L,creation_date#2] Batched: true, DataFilters: [isnotnull(question_id#0L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the adaptive = true\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# initialize start_time\n",
    "start_time = time.time()\n",
    "\n",
    "answers_month = answersDF.withColumn('month', month('creation_date'))\n",
    "\n",
    "# Repartition answers_month by month\n",
    "answers_month = answers_month.repartition(col(\"month\"))\n",
    "\n",
    "answers_month = answers_month.groupBy('question_id', 'month').agg(count('*').alias('cnt'))\n",
    "\n",
    "resultDF_1 = questionsDF.join(answers_month, \"question_id\").select('question_id', 'creation_date', 'title', 'month', 'cnt')\n",
    "resultDF_1.orderBy('question_id', 'month').show()\n",
    "\n",
    "print(\"Processing time: %s seconds\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "resultDF_1.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 2\n",
    "- Using `broadcast()` function to optimize the execution plan. Since Spark splits up data on different nodes in a cluster so multiple computers can process data in parallel. traditional joins are less optimal with Spark because the data is split.\n",
    "- `Broadcast joins` are easier to run on a cluster. Spark can `broadcast` a small DataFrame by sending all the data in the small DataFrame to all nodes in the cluster. After the small DataFrame is broadcasted, Spark can perform a join without shuffling more data in the larger DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|question_id|       creation_date|               title|month|cnt|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    1|  1|\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    2|  1|\n",
      "|     155990|2014-12-31 21:51:...|The abstract spac...|    1|  2|\n",
      "|     155992|2014-12-31 22:44:...|centrifugal force...|    1|  1|\n",
      "|     155993|2014-12-31 22:56:...|How can I estimat...|    1|  1|\n",
      "|     155995|2015-01-01 00:16:...|Why should a solu...|    1|  3|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    1|  2|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    2|  1|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|   11|  1|\n",
      "|     155997|2015-01-01 01:26:...|Why do square sha...|    1|  3|\n",
      "|     155999|2015-01-01 02:01:...|Diagonalizability...|    1|  1|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|    1|  2|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|   11|  1|\n",
      "|     156016|2015-01-01 05:31:...|The interference ...|    1|  1|\n",
      "|     156020|2015-01-01 06:19:...|What is going on ...|    1|  1|\n",
      "|     156021|2015-01-01 06:21:...|How to calculate ...|    2|  1|\n",
      "|     156022|2015-01-01 06:55:...|Advice on Major S...|    1|  1|\n",
      "|     156025|2015-01-01 07:32:...|Deriving the Cano...|    1|  1|\n",
      "|     156026|2015-01-01 07:49:...|Does Bell's inequ...|    1|  3|\n",
      "|     156027|2015-01-01 07:49:...|Deriving X atom f...|    1|  1|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Processing time: 0.6551077365875244 seconds\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [question_id#12L, creation_date#14, title#15, month#210, cnt#226L]\n",
      "   +- BroadcastHashJoin [question_id#12L], [question_id#0L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(question_id#12L)\n",
      "      :  +- FileScan parquet [question_id#12L,creation_date#14,title#15] Batched: true, DataFilters: [isnotnull(question_id#12L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [id=#611]\n",
      "         +- HashAggregate(keys=[question_id#0L, month#210], functions=[count(1)])\n",
      "            +- Exchange hashpartitioning(question_id#0L, month#210, 200), ENSURE_REQUIREMENTS, [id=#608]\n",
      "               +- HashAggregate(keys=[question_id#0L, month#210], functions=[partial_count(1)])\n",
      "                  +- Project [question_id#0L, month(cast(creation_date#2 as date)) AS month#210]\n",
      "                     +- Filter isnotnull(question_id#0L)\n",
      "                        +- FileScan parquet [question_id#0L,creation_date#2] Batched: true, DataFilters: [isnotnull(question_id#0L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the adaptive = true\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "answers_month = answersDF.withColumn('month', month('creation_date')).groupBy('question_id', 'month').agg(count('*').alias('cnt'))\n",
    "\n",
    "# using broadcast() with \"answers_month\"\n",
    "resultDF_2 = questionsDF.join(broadcast(answers_month), \"question_id\").select('question_id', 'creation_date', 'title', 'month', 'cnt')\n",
    "\n",
    "resultDF_2.orderBy('question_id', 'month').show()\n",
    "\n",
    "print(\"Processing time: %s seconds\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "resultDF_2.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 3\n",
    "- We are combining `repartition` and `broadcast` into the query to optimize the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|question_id|       creation_date|               title|month|cnt|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    1|  1|\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    2|  1|\n",
      "|     155990|2014-12-31 21:51:...|The abstract spac...|    1|  2|\n",
      "|     155992|2014-12-31 22:44:...|centrifugal force...|    1|  1|\n",
      "|     155993|2014-12-31 22:56:...|How can I estimat...|    1|  1|\n",
      "|     155995|2015-01-01 00:16:...|Why should a solu...|    1|  3|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    1|  2|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    2|  1|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|   11|  1|\n",
      "|     155997|2015-01-01 01:26:...|Why do square sha...|    1|  3|\n",
      "|     155999|2015-01-01 02:01:...|Diagonalizability...|    1|  1|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|    1|  2|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|   11|  1|\n",
      "|     156016|2015-01-01 05:31:...|The interference ...|    1|  1|\n",
      "|     156020|2015-01-01 06:19:...|What is going on ...|    1|  1|\n",
      "|     156021|2015-01-01 06:21:...|How to calculate ...|    2|  1|\n",
      "|     156022|2015-01-01 06:55:...|Advice on Major S...|    1|  1|\n",
      "|     156025|2015-01-01 07:32:...|Deriving the Cano...|    1|  1|\n",
      "|     156026|2015-01-01 07:49:...|Does Bell's inequ...|    1|  3|\n",
      "|     156027|2015-01-01 07:49:...|Deriving X atom f...|    1|  1|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Processing time: 0.5135302543640137 seconds\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [question_id#12L, creation_date#14, title#15, month#268, cnt#284L]\n",
      "   +- BroadcastHashJoin [question_id#12L], [question_id#0L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(question_id#12L)\n",
      "      :  +- FileScan parquet [question_id#12L,creation_date#14,title#15] Batched: true, DataFilters: [isnotnull(question_id#12L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [id=#816]\n",
      "         +- HashAggregate(keys=[question_id#0L, month#268], functions=[count(1)])\n",
      "            +- HashAggregate(keys=[question_id#0L, month#268], functions=[partial_count(1)])\n",
      "               +- Exchange hashpartitioning(month#268, 200), REPARTITION_BY_COL, [id=#808]\n",
      "                  +- Project [question_id#0L, month(cast(creation_date#2 as date)) AS month#268]\n",
      "                     +- Filter isnotnull(question_id#0L)\n",
      "                        +- FileScan parquet [question_id#0L,creation_date#2] Batched: true, DataFilters: [isnotnull(question_id#0L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the adaptive = true\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# initialize start_time\n",
    "start_time = time.time()\n",
    "\n",
    "answers_month = answersDF.withColumn('month', month('creation_date'))\n",
    "\n",
    "# Repartition answers_month by month\n",
    "answers_month = answers_month.repartition(col(\"month\"))\n",
    "\n",
    "answers_month = answers_month.groupBy('question_id', 'month').agg(count('*').alias('cnt'))\n",
    "\n",
    "resultDF_3 = questionsDF.join(broadcast(answers_month), \"question_id\").select('question_id', 'creation_date', 'title', 'month', 'cnt')\n",
    "\n",
    "resultDF_3.orderBy('question_id', 'month').show()\n",
    "\n",
    "print(\"Processing time: %s seconds\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "resultDF_3.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 4\n",
    "- If we look at the questionsDF DataFrame, we will see the column `accepted_answer_id` so let assume that we only take the accepted answers in the consideration and filter out all the answers that have `null` value. \n",
    "- We also repartition the answersDF with column `month` as well as using `broadcast()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|question_id|       creation_date|               title|month|cnt|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    1|  1|\n",
      "|     155989|2014-12-31 20:59:...|Frost bubble form...|    2|  1|\n",
      "|     155990|2014-12-31 21:51:...|The abstract spac...|    1|  2|\n",
      "|     155995|2015-01-01 00:16:...|Why should a solu...|    1|  3|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    1|  2|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|    2|  1|\n",
      "|     155996|2015-01-01 01:06:...|Why do we assume ...|   11|  1|\n",
      "|     155997|2015-01-01 01:26:...|Why do square sha...|    1|  3|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|    1|  2|\n",
      "|     156008|2015-01-01 03:48:...|Capturing a light...|   11|  1|\n",
      "|     156022|2015-01-01 06:55:...|Advice on Major S...|    1|  1|\n",
      "|     156025|2015-01-01 07:32:...|Deriving the Cano...|    1|  1|\n",
      "|     156027|2015-01-01 07:49:...|Deriving X atom f...|    1|  1|\n",
      "|     156050|2015-01-01 10:40:...|Position of Neutr...|    1|  2|\n",
      "|     156051|2015-01-01 10:43:...|Relation between ...|    1|  1|\n",
      "|     156055|2015-01-01 10:59:...|Meaning of revers...|    1|  1|\n",
      "|     156055|2015-01-01 10:59:...|Meaning of revers...|    4|  1|\n",
      "|     156055|2015-01-01 10:59:...|Meaning of revers...|   10|  1|\n",
      "|     156058|2015-01-01 11:31:...|are protons in pr...|    1|  1|\n",
      "|     156093|2015-01-01 17:09:...|Quartz Clock Does...|    1|  1|\n",
      "+-----------+--------------------+--------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Processing time: 0.42055773735046387 seconds\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [question_id#326L, creation_date#340, title#341, month#354, cnt#370L]\n",
      "   +- BroadcastHashJoin [question_id#326L], [question_id#338L], Inner, BuildRight, false\n",
      "      :- HashAggregate(keys=[question_id#326L, month#354], functions=[count(1)])\n",
      "      :  +- HashAggregate(keys=[question_id#326L, month#354], functions=[partial_count(1)])\n",
      "      :     +- Exchange hashpartitioning(month#354, 200), REPARTITION_BY_COL, [id=#990]\n",
      "      :        +- Project [question_id#326L, month(cast(creation_date#328 as date)) AS month#354]\n",
      "      :           +- Filter isnotnull(question_id#326L)\n",
      "      :              +- FileScan parquet [question_id#326L,creation_date#328] Batched: true, DataFilters: [isnotnull(question_id#326L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [id=#999]\n",
      "         +- Project [question_id#338L, creation_date#340, title#341]\n",
      "            +- Filter (isnotnull(accepted_answer_id#342L) AND isnotnull(question_id#338L))\n",
      "               +- FileScan parquet [question_id#338L,creation_date#340,title#341,accepted_answer_id#342L] Batched: true, DataFilters: [isnotnull(accepted_answer_id#342L), isnotnull(question_id#338L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Volumes/Moon/SpringBoard/spark-optimization/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(accepted_answer_id), IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string,accepted_answer_id:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the adaptive = true\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# initialize start_time\n",
    "start_time = time.time()\n",
    "\n",
    "answersDF = spark.read.option('path', answers_input_path).load()\n",
    "\n",
    "# filter out all the answers that have null value\n",
    "questionsDF = spark.read.option('path', questions_input_path).load().filter(col(\"accepted_answer_id\").isNotNull())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "answers_month = answersDF.withColumn('month', month('creation_date')).repartition(col(\"month\")).groupBy('question_id', 'month').agg(count('*').alias('cnt'))\n",
    "\n",
    "resultDF_4 = answers_month.join(broadcast(questionsDF), \"question_id\").select('question_id', 'creation_date', 'title', 'month', 'cnt')\n",
    "\n",
    "resultDF_4.orderBy('question_id', 'month').show()\n",
    "\n",
    "print(\"Processing time: %s seconds\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "resultDF_4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "- As we can see from the above queries, **solution 3** give us better performance with the Processing time: `0.5135302543640137 seconds`.\n",
    "\n",
    "- And if we only take the `accepted_answer_id` in the consideration, the **solution 4** gives us the optimal performance above all with the Processing time: `0.42055773735046387 seconds`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
